{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanaRFatec/Processamento-de-Linguagem-Neural/blob/master/Quarta%20Aula.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 4 - Limpeza de dados textuais"
      ],
      "metadata": {
        "id": "5I7jDRVS8Xbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Normalização de texto e Remoçaõ de ruído\n",
        "\n",
        "*   Remover caracteres especiais, pontuações, e normalizar o uso de letras maiúsculas e minúsculas\n",
        "\n"
      ],
      "metadata": {
        "id": "8aG_mLLKXjmN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqmbkHE38Pd4",
        "outputId": "3b530002-4045-4a4e-81ab-ba26130990b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Olá!!! Este é um exemplo de texto, com várias PONTUAÇÕES, símbolos #especiais, e LETRAS maiúsculas.\n",
            "\n",
            "Texto limpo: Olá Este é um exemplo de texto com várias PONTUAÇÕES símbolos especiais e LETRAS maiúsculas\n",
            "\n",
            "Texto normalizado: olá este é um exemplo de texto com várias pontuações símbolos especiais e letras maiúsculas\n"
          ]
        }
      ],
      "source": [
        "# importar a biblioteca que tras as funcionalidades para expressões regulares\n",
        "import re\n",
        "\n",
        "original = \"Olá!!! Este é um exemplo de texto, com várias PONTUAÇÕES, símbolos #especiais, e LETRAS maiúsculas.\"\n",
        "\n",
        "texto_limpo = re.sub(r'[^A-Za-zÀ-ÿ\\s]', '', original)\n",
        "  # realiza a substituicação dos caracteres especiais\n",
        "  # re.sub(parametro1, parametro2, parametro3) >>> realizar a busca e a substituição\n",
        "    # parametro1: r'[^A-Za-zÀ-ÿ\\s]': é o trecho que irá ser buscado para substituir\n",
        "      # ^A-Za-zÀ-ÿ\\s: os intervalos de texto que serão procurados\n",
        "        # A-Z: busca o intervalo de letras de A até Z\n",
        "        # a-z: busca o intervalo de letras de a até z\n",
        "        # À-ÿ: busca qualquer letra acentuada\n",
        "        # ^: representa a negação dos valores\n",
        "        # [^A-Za-zÀ-ÿ\\s]: busca todos os valores que não são letras (com ou sem acento)\n",
        "        # \\s: deixar os espaços\n",
        "    # parametro2: '' >>> o termo que eu vou substituir, no caso uma string vazia\n",
        "    # parametro3: variável que contém o meu texto\n",
        "    # O QUE É O TERMO R NA EXPRESSÃO REGULAR REGEX - OLHAR NA DOCUMENTAÇÃO\n",
        "\n",
        "texto_normalizado = texto_limpo.lower()\n",
        "\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\nTexto limpo: {texto_limpo}')\n",
        "print(f'\\nTexto normalizado: {texto_normalizado}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Tokenização\n",
        "*   Tokenização é dividir o texto em unidades menores (tokens), que geralmente são palavras ou pontuações.\n",
        "\n"
      ],
      "metadata": {
        "id": "ityE6BRiFuuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokens = word_tokenize(texto_normalizado)\n",
        "\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phRmuGlcGA9g",
        "outputId": "e4da29d7-522b-401b-ab9a-741c7e86fdea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['olá', 'este', 'é', 'um', 'exemplo', 'de', 'texto', 'com', 'várias', 'pontuações', 'símbolos', 'especiais', 'e', 'letras', 'maiúsculas']\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Remoção de StopWords\n",
        "\n",
        "*   StopWords são palavras de pouco valor semântico (como \"de\", \"a\", \"o\") que podem ser removidas para simplificar o texto\n",
        "\n"
      ],
      "metadata": {
        "id": "8NGNvrr8X1TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords_pt = set (stopwords.words('portuguese'))\n",
        "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]\n",
        "print(tokens_sem_stopwords)\n",
        "print(len(tokens_sem_stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JzGHqfhP7ue",
        "outputId": "d07df063-0a4b-41d9-e9ce-c4d9e40a4c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['olá', 'exemplo', 'texto', 'várias', 'pontuações', 'símbolos', 'especiais', 'letras', 'maiúsculas']\n",
            "9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Stemming e Lematização\n",
        "\n",
        "*   Stemming reduz as palavras às suas raízes (ou radicais);\n",
        "*   Lematização normaliza as palvras para suas formas base, levando em conta contexto e gramática - ESTE EXEMPLO\n",
        "\n"
      ],
      "metadata": {
        "id": "bZKhRcS9V9Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "nltk.download('rslp')\n",
        "\n",
        "stemmer = RSLPStemmer()\n",
        "stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
        "print(stemming)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntorHrcSWBx2",
        "outputId": "d8f8abc5-c842-45ad-968d-d073aee4ac03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['olá', 'exempl', 'text', 'vár', 'pontu', 'símbol', 'espec', 'letr', 'maiúscul']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Utilizando todo o processo de hoje"
      ],
      "metadata": {
        "id": "rjzrvKCzYWJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importação das bibliotecas - por ordem alfabética\n",
        "import nltk\n",
        "import re\n",
        "# importação das funcionalidades\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download dos recursos do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Texto de exemplo\n",
        "texto = \"Este é um exemplo de texto com muitas palavras, algumas repetidas outra @não, e pontuação e outros simbolos que não FAzem parte do texto que desejo . Vamos limpar e organizar esse texto. Há também números 123 e caracteres especiais @#$%.\"\n",
        "\n",
        "# Limpeza de ruídos e normalização\n",
        "texto_limpo = re.sub(r'[^a-zA-Z]', ' ', texto)\n",
        "texto_lower = texto_limpo.lower()\n",
        "\n",
        "# Tokenização\n",
        "tokens = nltk.word_tokenize(texto_lower)\n",
        "\n",
        "# Remoção de stopwords\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "palavras_filtradas = [palavra for palavra in tokens if palavra not in stop_words]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "palavras_stemizadas = [stemmer.stem(palavra) for palavra in palavras_filtradas]\n",
        "\n",
        "# Impressão do resultado final\n",
        "print(palavras_stemizadas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNAyYZGCZ3q1",
        "outputId": "2d9f8009-2200-49f3-deec-f5b0ee4ee0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'muita', 'palavra', 'alguma', 'repetida', 'outra', 'n', 'pontua', 'outro', 'simbolo', 'n', 'fazem', 'part', 'texto', 'desejo', 'vamo', 'limpar', 'organizar', 'texto', 'h', 'tamb', 'm', 'n', 'mero', 'caracter', 'especiai']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 05**"
      ],
      "metadata": {
        "id": "ihIO7tqPkVkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documentos = [\n",
        "    \"gato e cachorro.\",\n",
        "    \"gato brinca com cachorro.\",\n",
        "    \"gato e rato.\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(documentos)\n",
        "\n",
        "print(\"Matriz BoW:\")\n",
        "print(X.toarray())\n",
        "\n",
        "print(f\"Vocabulario: {vectorizer.vocabulary_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "ZIbXjFgpkZle",
        "outputId": "7da9405d-e908-4306-9f61-c2a87bb25aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz BoW:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9f3ad1cedcaa>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Matriz BoW:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Vocabulario: {vectorizer.vocabulary_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documentos = [\n",
        "    \"gato e cachorro.\",\n",
        "    \"gato brinca com cachorro.\",\n",
        "    \"gato e rato.\"\n",
        "]\n",
        "\n",
        "vectorizer_bow = CountVectorizer()\n",
        "x_bow = vectorizer_bow.fit_transform(documentos)\n",
        "\n",
        "\n",
        "print(f\"Vocabulario BoW: {vectorizer_bow.vocabulary_}\")\n",
        "print(\"Matriz BoW:\")\n",
        "print(x_bow.toarray())\n",
        "\n",
        "\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "x_tfidf = vectorizer_tfidf.fit_transform(documentos)\n",
        "\n",
        "print(f\"Vocabulario TF-IDF: {vectorizer_tfidf.vocabulary_}\")\n",
        "print(\"Matriz TF-IDF:\")\n",
        "print(x_tfidf.toarray())"
      ],
      "metadata": {
        "id": "zLXJaxqMld8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}